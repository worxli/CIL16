{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Baseline for CIL project on road segmentation.\n",
    "This simple baseline consits of a CNN with two convolutional+pooling layers with a soft-max loss\n",
    "\"\"\"\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from random import shuffle\n",
    "import random\n",
    "\n",
    "import code\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import cil_helper as cil\n",
    "\n",
    "from skimage.feature import greycomatrix, greycoprops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tf.app.flags.DEFINE_string('log_dir', '/tmp/tensorflow_lukas',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "except: \n",
    "    print(tf.app.flags.FLAGS.log_dir)\n",
    "    \n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 50\n",
    "VALIDATION_SIZE = 5  # Size of the validation set.\n",
    "TEST_SIZE = 20\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 64 # 64\n",
    "NUM_EPOCHS = 200\n",
    "RESTORE_MODEL = False # If True, restore existing model instead of training a new one\n",
    "RECORDING_STEP = 10\n",
    "\n",
    "# Set image patch size in pixels\n",
    "# IMG_PATCH_SIZE should be a multiple of 4\n",
    "# image size should be an integer multiple of this number!\n",
    "\n",
    "PATCH_SIZE = 16\n",
    "EXTRA_CONTEXT = 16\n",
    "CONTEXT_SIZE = EXTRA_CONTEXT + PATCH_SIZE + EXTRA_CONTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'training/'\n",
    "train_data_filename = data_dir + 'images/'\n",
    "train_labels_filename = data_dir + 'groundtruth/' \n",
    "test_dir = \"test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns all train images in rgb\n",
    "def get_train_imgs():\n",
    "    imgs = []\n",
    "    for i in range(TRAINING_SIZE):\n",
    "        imageid = \"satImage_%.3d\" % (i+1)\n",
    "        image_filename = train_data_filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            #print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "        else:\n",
    "            raise Exception('File ' + image_filename + ' does not exist')\n",
    "    return np.asarray(imgs)\n",
    "\n",
    "# returns all test images as rgb\n",
    "def get_test_imgs():\n",
    "    imgs = []\n",
    "    for i in range(TEST_SIZE):\n",
    "        imagename = \"test_\" + str(i+1)\n",
    "        image_filename = test_dir + imagename + \"/\" + imagename + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "        else:\n",
    "            raise Exception('File ' + image_filename + ' does not exist')\n",
    "    return np.asarray(imgs)\n",
    "\n",
    "# Get prediction overlaid on the original image for given input image\n",
    "def get_prediction_with_overlay(img):\n",
    "    \n",
    "    img_prediction = get_prediction(img)\n",
    "    oimg = cil.make_img_overlay(img[:,:,:3], img_prediction)\n",
    "\n",
    "    return oimg\n",
    "\n",
    "# Get a concatenation of the prediction and groundtruth for given input file\n",
    "def get_prediction_with_groundtruth(img):\n",
    "    \n",
    "    img_prediction = get_prediction(img)\n",
    "    cimg = cil.concatenate_images(img[:,:,:3], img_prediction)\n",
    "\n",
    "    return cimg\n",
    "\n",
    "# Get prediction for given input image \n",
    "def get_prediction(img):\n",
    "    data = np.asarray(cil.img_crop(img, IMG_PATCH_SIZE, IMG_PATCH_SIZE))\n",
    "    data_node = tf.constant(data)\n",
    "    output = tf.nn.softmax(model(data_node))\n",
    "    output_prediction = s.run(output)\n",
    "    img_prediction = cil.label_to_img(img.shape[0], img.shape[1], IMG_PATCH_SIZE, IMG_PATCH_SIZE, output_prediction)\n",
    "\n",
    "    return img_prediction\n",
    "\n",
    "# returns a numpy array [Saturation, Lightness] of a pixel\n",
    "def sat_light(rgb):\n",
    "    Cmax = rgb.max()\n",
    "    Cmin = rgb.min()\n",
    "    d = Cmax - Cmin\n",
    "    L = (Cmax + Cmin) / 2\n",
    "    S = d / (1 - (abs(2 * L - 1)))\n",
    "    if np.isnan(S):\n",
    "        S = 0\n",
    "    return np.asarray([S, L], dtype=np.float32)\n",
    "\n",
    "#returns edges\n",
    "def texture_features(image):\n",
    "    result = np.empty(list(image.shape) + [2])\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]\n",
    "    result[:,:,0] = np.vstack((image[:-1,:] - image[1:,:], np.zeros((1,w))))\n",
    "    result[:,:,1] = np.hstack((image[:,:-1] - image[:,1:], np.zeros((h,1))))\n",
    "    return np.max(np.abs(result),axis=2)\n",
    "\n",
    "NUM_CHANNELS = 6 # RGB, Saturation, Lightness, Edges\n",
    "def preprocess(img):\n",
    "    new_shape = list(img.shape)\n",
    "    new_shape[2] = NUM_CHANNELS\n",
    "    result = np.empty(new_shape, dtype=np.float32)\n",
    "    result[:,:,:3] = img\n",
    "    for y in range(len(img)):\n",
    "        for x in range(len(img[y])):\n",
    "            result[y,x,3:5] = sat_light(result[y,x,:3])\n",
    "    #print('Saturation and lightness computed.')\n",
    "    result[:,:,5] = texture_features(result[:,:,4])\n",
    "    #print('Texture deatures computed.')\n",
    "    return result\n",
    "\n",
    "# takes an array of images, preprocesses them and stores the result\n",
    "# in save_dir/preprocessed.npy\n",
    "def preprocess_imgs(imgs, save_dir):\n",
    "    assert save_dir.endswith('/')\n",
    "    processed = []\n",
    "    for i in range(len(imgs)):\n",
    "        processed.append(preprocess(imgs[i]))\n",
    "        print('Image ' + str(i) + ' preprocessed.')\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    processed_np = np.asarray(processed)\n",
    "    np.save(save_dir + 'preprocessed2.npy', processed_np)\n",
    "    return processed_np\n",
    "\n",
    "def load_preproc(save_dir):\n",
    "    assert save_dir.endswith('/')\n",
    "    return np.load(save_dir + 'preprocessed2.npy')\n",
    "\n",
    "def load_labels():\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    for i in range(1, TRAINING_SIZE+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = train_labels_filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            #print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            gt_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(gt_imgs)\n",
    "    labels = np.asarray(gt_imgs, dtype=np.float32)\n",
    "    return labels\n",
    "\n",
    "def store_channel_to_img(chan):\n",
    "    img = (chan * 255).astype('uint8')\n",
    "    png = Image.fromarray(img)\n",
    "    png.show\n",
    "    return png\n",
    "\n",
    "def show(chan):\n",
    "    chan = chan + chan.min()\n",
    "    chan = chan / chan.max()\n",
    "    img = (chan * 255).astype('uint8')\n",
    "    png = Image.fromarray(img)\n",
    "    png.show\n",
    "    return png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PREPROCESS_TRAIN_DATA = False\n",
    "PREPROCESS_TEST_DATA = False\n",
    "\n",
    "if PREPROCESS_TRAIN_DATA:\n",
    "    preprocess_imgs(get_train_imgs(), train_data_filename)\n",
    "if PREPROCESS_TEST_DATA:\n",
    "    preprocess_imgs(get_test_imgs(), test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 400, 400, 6)\n",
      "(50, 400, 400)\n"
     ]
    }
   ],
   "source": [
    "train_data = load_preproc(train_data_filename)\n",
    "train_labels = load_labels()\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is where training samples and labels are fed to the graph.\n",
    "# These placeholder nodes will be fed a batch of training data at each\n",
    "# training step using the {feed_dict} argument to the Run() call below.\n",
    "train_data_node = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(BATCH_SIZE, CONTEXT_SIZE, CONTEXT_SIZE, NUM_CHANNELS),\n",
    "    name='train_data')\n",
    "train_labels_node = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(BATCH_SIZE, cil.NUM_LABELS),\n",
    "    name='train_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILTER1_SIZE = 5\n",
    "FILTER1_COUNT = 32\n",
    "FILTER1_STRIDES = [1,1,1,1]\n",
    "\n",
    "POOL1_FACTOR = 2\n",
    "\n",
    "FILTER2_SIZE = 5\n",
    "FILTER2_COUNT = 64\n",
    "FILTER2_STRIDES = [1,1,1,1]\n",
    "\n",
    "POOL2_FACTOR = 2\n",
    "\n",
    "FILTER3_SIZE = 3\n",
    "FILTER3_COUNT = 64\n",
    "FILTER3_STRIDES = [1,1,1,1]\n",
    "\n",
    "POOL3_FACTOR = 1\n",
    "\n",
    "FC1_SIZE = 512\n",
    "FC2_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter1_shape = [FILTER1_SIZE, FILTER1_SIZE, NUM_CHANNELS, FILTER1_COUNT]\n",
    "filter1 = tf.Variable(tf.truncated_normal(filter1_shape, stddev=.1, seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros(shape=[FILTER1_COUNT]))\n",
    "\n",
    "filter2_shape = [FILTER2_SIZE, FILTER2_SIZE, FILTER1_COUNT, FILTER2_COUNT]\n",
    "filter2 = tf.Variable(tf.truncated_normal(filter2_shape, stddev=.1, seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(.1, shape=[FILTER2_COUNT]))\n",
    "\n",
    "filter3_shape = [FILTER3_SIZE, FILTER3_SIZE, FILTER2_COUNT, FILTER3_COUNT]\n",
    "filter3 = tf.Variable(tf.truncated_normal(filter3_shape, stddev=.1, seed=SEED))\n",
    "conv3_biases = tf.Variable(tf.constant(.1, shape=[FILTER3_COUNT]))\n",
    "\n",
    "side = (((CONTEXT_SIZE // POOL1_FACTOR) // POOL2_FACTOR) // POOL3_FACTOR)\n",
    "reshape_depth = side * side * FILTER3_COUNT\n",
    "fc1_weights = tf.Variable(\n",
    "    tf.truncated_normal([reshape_depth, FC1_SIZE], stddev=.1, seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[FC1_SIZE]))\n",
    "\n",
    "fc2_weights = tf.Variable(\n",
    "    tf.truncated_normal([FC1_SIZE, FC2_SIZE], stddev=.1, seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[FC2_SIZE]))\n",
    "\n",
    "fc_last_weights = tf.Variable(\n",
    "    tf.truncated_normal([FC2_SIZE, cil.NUM_LABELS], stddev=.1, seed=SEED))\n",
    "fc_last_biases = tf.Variable(tf.constant(0.1, shape=[cil.NUM_LABELS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(input_node, dropout=False):\n",
    "    print(input_node.get_shape())\n",
    "\n",
    "    conv1 = tf.nn.conv2d(input_node, filter1, FILTER1_STRIDES, \n",
    "                         padding='SAME', name='conv1')\n",
    "\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "\n",
    "    pool1 = tf.nn.max_pool(relu1, ksize=[1,POOL1_FACTOR,POOL1_FACTOR,1], \n",
    "                           strides=[1,POOL1_FACTOR,POOL1_FACTOR,1], padding='SAME')\n",
    "    print(pool1.get_shape())\n",
    "    ######################\n",
    "    conv2 = tf.nn.conv2d(pool1, filter2, FILTER2_STRIDES, \n",
    "                         padding='SAME', name='conv2')\n",
    "\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1,POOL2_FACTOR,POOL2_FACTOR,1], \n",
    "                           strides=[1,POOL2_FACTOR,POOL2_FACTOR,1], padding='SAME')\n",
    "    print(pool2.get_shape())\n",
    "    ######################\n",
    "    conv3 = tf.nn.conv2d(pool2, filter3, FILTER3_STRIDES, \n",
    "                         padding='SAME', name='conv2')\n",
    "\n",
    "    relu3 = tf.nn.relu(tf.nn.bias_add(conv3, conv3_biases))\n",
    "\n",
    "    pool3 = tf.nn.max_pool(relu3, ksize=[1,POOL3_FACTOR,POOL3_FACTOR,1], \n",
    "                           strides=[1,POOL3_FACTOR,POOL3_FACTOR,1], padding='SAME')\n",
    "    print(pool3.get_shape())\n",
    "    ######################\n",
    "    shp = pool3.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool3, [shp[0], shp[1] * shp[2] * shp[3]])\n",
    "    print(reshape.get_shape())\n",
    "\n",
    "    fc1 = tf.nn.sigmoid(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "    if (dropout):\n",
    "        fc1 = tf.nn.dropout(fc1, .5, seed=SEED)\n",
    "    print(fc1.get_shape())\n",
    "\n",
    "    fc2 = tf.nn.sigmoid(tf.matmul(fc1, fc2_weights) + fc2_biases)\n",
    "    if (dropout):\n",
    "        fc2 = tf.nn.dropout(fc2, .5, seed=SEED)\n",
    "    print(fc2.get_shape())\n",
    "\n",
    "    fc_last = tf.matmul(fc2, fc_last_weights) + fc_last_biases\n",
    "    print(fc_last.get_shape())\n",
    "    return fc_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 48, 48, 6)\n",
      "(64, 24, 24, 32)\n",
      "(64, 12, 12, 64)\n",
      "(64, 12, 12, 64)\n",
      "(64, 9216)\n",
      "(64, 512)\n",
      "(64, 128)\n",
      "(64, 2)\n"
     ]
    }
   ],
   "source": [
    "logits = model(train_data_node, dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, train_labels_node))\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases) +\n",
    "                tf.nn.l2_loss(fc_last_weights) + tf.nn.l2_loss(fc_last_biases))\n",
    "loss += 5e-4 * regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DECAY_STEP = 5000\n",
    "batch_idx = tf.Variable(0)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    0.01,                # Base learning rate.\n",
    "    batch_idx * BATCH_SIZE,  # Current index into the dataset.\n",
    "    DECAY_STEP,          # Decay step.\n",
    "    0.95,                # Decay rate.\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "#    loss, global_step=batch_idx)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, 0.0).minimize(\n",
    "    loss, global_step=batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_window(image, y, x, truth=None):\n",
    "    #padding the image with wrap-around\n",
    "    window = np.take(np.take(image, \n",
    "                     range(y,y+CONTEXT_SIZE), axis=0,mode='wrap'), \n",
    "                     range(x,x+CONTEXT_SIZE), axis=1,mode='wrap')\n",
    "    if not truth is None:\n",
    "        truth_window = np.take(np.take(truth,\n",
    "     range(y+EXTRA_CONTEXT,y+EXTRA_CONTEXT+PATCH_SIZE), axis=0,mode='wrap'),\n",
    "     range(x+EXTRA_CONTEXT,x+EXTRA_CONTEXT+PATCH_SIZE), axis=1,mode='wrap')\n",
    "        return (window, truth_window)\n",
    "    else:\n",
    "        return window\n",
    "\n",
    "def get_training_samples(batch_size):\n",
    "    batch = np.empty((batch_size, CONTEXT_SIZE, CONTEXT_SIZE, NUM_CHANNELS))\n",
    "    labels = np.empty((batch_size, cil.NUM_LABELS))\n",
    "    for i in range(batch_size):\n",
    "        pic_idx = random.randrange(0,TRAINING_SIZE)\n",
    "        pic = train_data[pic_idx]\n",
    "        y = random.randrange(0,pic.shape[0])\n",
    "        x = random.randrange(0,pic.shape[1])\n",
    "        \n",
    "        window, truth = get_window(pic, y, x, train_labels[pic_idx])\n",
    "        label = int(truth.mean() > .25)\n",
    "        batch[i] = window\n",
    "        labels[i][0] = label\n",
    "        labels[i][1] = 1 - label\n",
    "    return (batch, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n"
     ]
    }
   ],
   "source": [
    "s.run(tf.initialize_all_variables())\n",
    "print ('Initialized!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Loss: 9.85099\n",
      "Error: 25.0\n",
      "Epoch 1:\n",
      "Loss: 9.83331\n",
      "Error: 26.5625\n",
      "Epoch 2:\n",
      "Loss: 9.97312\n",
      "Error: 35.9375\n",
      "Epoch 3:\n",
      "Loss: 9.77931\n",
      "Error: 23.4375\n",
      "Epoch 4:\n",
      "Loss: 9.91388\n",
      "Error: 35.9375\n",
      "Epoch 5:\n",
      "Loss: 9.70539\n",
      "Error: 14.0625\n",
      "Epoch 6:\n",
      "Loss: 9.75049\n",
      "Error: 17.1875\n",
      "Epoch 7:\n",
      "Loss: 9.78564\n",
      "Error: 20.3125\n",
      "Epoch 8:\n",
      "Loss: 9.88216\n",
      "Error: 35.9375\n",
      "Epoch 9:\n",
      "Loss: 9.84906\n",
      "Error: 23.4375\n",
      "Epoch 10:\n",
      "Loss: 9.99462\n",
      "Error: 35.9375\n",
      "Epoch 11:\n",
      "Loss: 9.8708\n",
      "Error: 25.0\n",
      "Epoch 12:\n",
      "Loss: 9.92734\n",
      "Error: 29.6875\n",
      "Epoch 13:\n",
      "Loss: 9.7297\n",
      "Error: 15.625\n",
      "Epoch 14:\n",
      "Loss: 9.72763\n",
      "Error: 20.3125\n",
      "Epoch 15:\n",
      "Loss: 9.93397\n",
      "Error: 34.375\n",
      "Epoch 16:\n",
      "Loss: 9.83087\n",
      "Error: 28.125\n",
      "Epoch 17:\n",
      "Loss: 9.77348\n",
      "Error: 20.3125\n",
      "Epoch 18:\n",
      "Loss: 9.78207\n",
      "Error: 23.4375\n",
      "Epoch 19:\n",
      "Loss: 9.89203\n",
      "Error: 37.5\n",
      "Epoch 20:\n",
      "Loss: 9.76946\n",
      "Error: 26.5625\n",
      "Epoch 21:\n",
      "Loss: 9.7726\n",
      "Error: 29.6875\n",
      "Epoch 22:\n",
      "Loss: 9.95996\n",
      "Error: 32.8125\n",
      "Epoch 23:\n",
      "Loss: 9.7597\n",
      "Error: 21.875\n",
      "Epoch 24:\n",
      "Loss: 9.84128\n",
      "Error: 31.25\n",
      "Epoch 25:\n",
      "Loss: 9.7333\n",
      "Error: 21.875\n",
      "Epoch 26:\n",
      "Loss: 9.91297\n",
      "Error: 31.25\n",
      "Epoch 27:\n",
      "Loss: 9.75862\n",
      "Error: 25.0\n",
      "Epoch 28:\n",
      "Loss: 9.80363\n",
      "Error: 26.5625\n",
      "Epoch 29:\n",
      "Loss: 9.87367\n",
      "Error: 32.8125\n",
      "Epoch 30:\n",
      "Loss: 9.81003\n",
      "Error: 23.4375\n",
      "Epoch 31:\n",
      "Loss: 9.6527\n",
      "Error: 12.5\n",
      "Epoch 32:\n",
      "Loss: 9.81582\n",
      "Error: 28.125\n",
      "Epoch 33:\n",
      "Loss: 9.69163\n",
      "Error: 15.625\n",
      "Epoch 34:\n",
      "Loss: 9.76799\n",
      "Error: 23.4375\n",
      "Epoch 35:\n",
      "Loss: 9.81134\n",
      "Error: 25.0\n",
      "Epoch 36:\n",
      "Loss: 9.7719\n",
      "Error: 23.4375\n",
      "Epoch 37:\n",
      "Loss: 9.81147\n",
      "Error: 25.0\n",
      "Epoch 38:\n",
      "Loss: 9.83911\n",
      "Error: 29.6875\n",
      "Epoch 39:\n",
      "Loss: 9.79206\n",
      "Error: 23.4375\n",
      "Epoch 40:\n",
      "Loss: 9.71931\n",
      "Error: 20.3125\n",
      "Epoch 41:\n",
      "Loss: 9.80737\n",
      "Error: 26.5625\n",
      "Epoch 42:\n",
      "Loss: 9.82128\n",
      "Error: 28.125\n",
      "Epoch 43:\n",
      "Loss: 9.81329\n",
      "Error: 28.125\n",
      "Epoch 44:\n",
      "Loss: 9.71139\n",
      "Error: 20.3125\n",
      "Epoch 45:\n",
      "Loss: 9.66484\n",
      "Error: 14.0625\n",
      "Epoch 46:\n",
      "Loss: 9.8775\n",
      "Error: 32.8125\n",
      "Epoch 47:\n",
      "Loss: 9.8006\n",
      "Error: 25.0\n",
      "Epoch 48:\n",
      "Loss: 9.77408\n",
      "Error: 28.125\n",
      "Epoch 49:\n",
      "Loss: 9.73718\n",
      "Error: 23.4375\n",
      "Epoch 50:\n",
      "Loss: 9.98627\n",
      "Error: 40.625\n",
      "Epoch 51:\n",
      "Loss: 9.8753\n",
      "Error: 31.25\n",
      "Epoch 52:\n",
      "Loss: 9.81127\n",
      "Error: 26.5625\n",
      "Epoch 53:\n",
      "Loss: 9.71316\n",
      "Error: 23.4375\n",
      "Epoch 54:\n",
      "Loss: 9.71703\n",
      "Error: 18.75\n",
      "Epoch 55:\n",
      "Loss: 9.76243\n",
      "Error: 25.0\n",
      "Epoch 56:\n",
      "Loss: 9.84754\n",
      "Error: 32.8125\n",
      "Epoch 57:\n",
      "Loss: 9.77219\n",
      "Error: 23.4375\n",
      "Epoch 58:\n",
      "Loss: 9.7079\n",
      "Error: 18.75\n",
      "Epoch 59:\n",
      "Loss: 9.74409\n",
      "Error: 26.5625\n",
      "Epoch 60:\n",
      "Loss: 9.76667\n",
      "Error: 25.0\n",
      "Epoch 61:\n",
      "Loss: 9.7802\n",
      "Error: 26.5625\n",
      "Epoch 62:\n",
      "Loss: 9.73592\n",
      "Error: 20.3125\n",
      "Epoch 63:\n",
      "Loss: 9.66296\n",
      "Error: 17.1875\n",
      "Epoch 64:\n",
      "Loss: 9.81082\n",
      "Error: 29.6875\n",
      "Epoch 65:\n",
      "Loss: 9.65261\n",
      "Error: 12.5\n",
      "Epoch 66:\n",
      "Loss: 9.79332\n",
      "Error: 25.0\n",
      "Epoch 67:\n",
      "Loss: 9.7014\n",
      "Error: 18.75\n",
      "Epoch 68:\n",
      "Loss: 9.78951\n",
      "Error: 23.4375\n",
      "Epoch 69:\n",
      "Loss: 9.7795\n",
      "Error: 26.5625\n",
      "Epoch 70:\n",
      "Loss: 9.90437\n",
      "Error: 37.5\n",
      "Epoch 71:\n",
      "Loss: 9.84734\n",
      "Error: 31.25\n",
      "Epoch 72:\n",
      "Loss: 9.72845\n",
      "Error: 21.875\n",
      "Epoch 73:\n",
      "Loss: 9.75891\n",
      "Error: 23.4375\n",
      "Epoch 74:\n",
      "Loss: 9.72943\n",
      "Error: 21.875\n",
      "Epoch 75:\n",
      "Loss: 9.78316\n",
      "Error: 28.125\n",
      "Epoch 76:\n",
      "Loss: 9.73952\n",
      "Error: 26.5625\n",
      "Epoch 77:\n",
      "Loss: 9.90515\n",
      "Error: 37.5\n",
      "Epoch 78:\n",
      "Loss: 9.84014\n",
      "Error: 32.8125\n",
      "Epoch 79:\n",
      "Loss: 9.62064\n",
      "Error: 12.5\n",
      "Epoch 80:\n",
      "Loss: 9.76407\n",
      "Error: 28.125\n",
      "Epoch 81:\n",
      "Loss: 9.79157\n",
      "Error: 29.6875\n",
      "Epoch 82:\n",
      "Loss: 9.75185\n",
      "Error: 23.4375\n",
      "Epoch 83:\n",
      "Loss: 9.74346\n",
      "Error: 25.0\n",
      "Epoch 84:\n",
      "Loss: 9.71771\n",
      "Error: 21.875\n",
      "Epoch 85:\n",
      "Loss: 9.91767\n",
      "Error: 39.0625\n",
      "Epoch 86:\n",
      "Loss: 9.83944\n",
      "Error: 34.375\n",
      "Epoch 87:\n",
      "Loss: 9.68379\n",
      "Error: 18.75\n",
      "Epoch 88:\n",
      "Loss: 9.76543\n",
      "Error: 25.0\n",
      "Epoch 89:\n",
      "Loss: 9.73652\n",
      "Error: 26.5625\n",
      "Epoch 90:\n",
      "Loss: 9.67331\n",
      "Error: 17.1875\n",
      "Epoch 91:\n",
      "Loss: 9.80205\n",
      "Error: 26.5625\n",
      "Epoch 92:\n",
      "Loss: 9.83202\n",
      "Error: 35.9375\n",
      "Epoch 93:\n",
      "Loss: 9.72182\n",
      "Error: 26.5625\n",
      "Epoch 94:\n",
      "Loss: 9.68466\n",
      "Error: 21.875\n",
      "Epoch 95:\n",
      "Loss: 9.75881\n",
      "Error: 25.0\n",
      "Epoch 96:\n",
      "Loss: 9.74792\n",
      "Error: 21.875\n",
      "Epoch 97:\n",
      "Loss: 9.72365\n",
      "Error: 20.3125\n",
      "Epoch 98:\n",
      "Loss: 9.82879\n",
      "Error: 34.375\n",
      "Epoch 99:\n",
      "Loss: 9.63417\n",
      "Error: 15.625\n",
      "Epoch 100:\n",
      "Loss: 9.75388\n",
      "Error: 26.5625\n",
      "Epoch 101:\n",
      "Loss: 9.81068\n",
      "Error: 29.6875\n",
      "Epoch 102:\n",
      "Loss: 9.78131\n",
      "Error: 26.5625\n",
      "Epoch 103:\n",
      "Loss: 9.78153\n",
      "Error: 32.8125\n",
      "Epoch 104:\n",
      "Loss: 9.73619\n",
      "Error: 26.5625\n",
      "Epoch 105:\n",
      "Loss: 9.73456\n",
      "Error: 23.4375\n",
      "Epoch 106:\n",
      "Loss: 9.66246\n",
      "Error: 20.3125\n",
      "Epoch 107:\n",
      "Loss: 9.74144\n",
      "Error: 26.5625\n",
      "Epoch 108:\n",
      "Loss: 9.78057\n",
      "Error: 29.6875\n",
      "Epoch 109:\n",
      "Loss: 9.69782\n",
      "Error: 18.75\n",
      "Epoch 110:\n",
      "Loss: 9.76684\n",
      "Error: 28.125\n",
      "Epoch 111:\n",
      "Loss: 9.75432\n",
      "Error: 29.6875\n",
      "Epoch 112:\n",
      "Loss: 9.63764\n",
      "Error: 17.1875\n",
      "Epoch 113:\n",
      "Loss: 9.76964\n",
      "Error: 26.5625\n",
      "Epoch 114:\n",
      "Loss: 9.77005\n",
      "Error: 28.125\n",
      "Epoch 115:\n",
      "Loss: 9.76997\n",
      "Error: 23.4375\n",
      "Epoch 116:\n",
      "Loss: 9.92871\n",
      "Error: 40.625\n",
      "Epoch 117:\n",
      "Loss: 9.75613\n",
      "Error: 25.0\n",
      "Epoch 118:\n",
      "Loss: 9.78396\n",
      "Error: 28.125\n",
      "Epoch 119:\n",
      "Loss: 9.7857\n",
      "Error: 29.6875\n",
      "Epoch 120:\n",
      "Loss: 9.66154\n",
      "Error: 21.875\n",
      "Epoch 121:\n",
      "Loss: 9.6974\n",
      "Error: 20.3125\n",
      "Epoch 122:\n",
      "Loss: 9.79521\n",
      "Error: 28.125\n",
      "Epoch 123:\n",
      "Loss: 9.73724\n",
      "Error: 21.875\n",
      "Epoch 124:\n",
      "Loss: 9.74048\n",
      "Error: 28.125\n",
      "Epoch 125:\n",
      "Loss: 9.82649\n",
      "Error: 34.375\n",
      "Epoch 126:\n",
      "Loss: 9.67522\n",
      "Error: 20.3125\n",
      "Epoch 127:\n",
      "Loss: 9.76602\n",
      "Error: 23.4375\n",
      "Epoch 128:\n",
      "Loss: 9.72595\n",
      "Error: 25.0\n",
      "Epoch 129:\n",
      "Loss: 9.83059\n",
      "Error: 32.8125\n",
      "Epoch 130:\n",
      "Loss: 9.69607\n",
      "Error: 20.3125\n",
      "Epoch 131:\n",
      "Loss: 9.86658\n",
      "Error: 35.9375\n",
      "Epoch 132:\n",
      "Loss: 9.80523\n",
      "Error: 31.25\n",
      "Epoch 133:\n",
      "Loss: 9.65239\n",
      "Error: 17.1875\n",
      "Epoch 134:\n",
      "Loss: 9.80229\n",
      "Error: 29.6875\n",
      "Epoch 135:\n",
      "Loss: 9.73067\n",
      "Error: 25.0\n",
      "Epoch 136:\n",
      "Loss: 9.62358\n",
      "Error: 18.75\n",
      "Epoch 137:\n",
      "Loss: 9.71893\n",
      "Error: 26.5625\n",
      "Epoch 138:\n",
      "Loss: 9.66911\n",
      "Error: 18.75\n",
      "Epoch 139:\n",
      "Loss: 9.76112\n",
      "Error: 28.125\n",
      "Epoch 140:\n",
      "Loss: 9.68881\n",
      "Error: 23.4375\n",
      "Epoch 141:\n",
      "Loss: 9.66832\n",
      "Error: 21.875\n",
      "Epoch 142:\n",
      "Loss: 9.75433\n",
      "Error: 29.6875\n",
      "Epoch 143:\n",
      "Loss: 9.81553\n",
      "Error: 31.25\n",
      "Epoch 144:\n",
      "Loss: 9.69395\n",
      "Error: 23.4375\n",
      "Epoch 145:\n",
      "Loss: 9.80305\n",
      "Error: 31.25\n",
      "Epoch 146:\n",
      "Loss: 9.80306\n",
      "Error: 32.8125\n",
      "Epoch 147:\n",
      "Loss: 9.61373\n",
      "Error: 18.75\n",
      "Epoch 148:\n",
      "Loss: 9.7536\n",
      "Error: 28.125\n",
      "Epoch 149:\n",
      "Loss: 9.70877\n",
      "Error: 21.875\n",
      "Epoch 150:\n",
      "Loss: 9.70281\n",
      "Error: 21.875\n",
      "Epoch 151:\n",
      "Loss: 9.82972\n",
      "Error: 32.8125\n",
      "Epoch 152:\n",
      "Loss: 9.70137\n",
      "Error: 18.75\n",
      "Epoch 153:\n",
      "Loss: 9.67669\n",
      "Error: 20.3125\n",
      "Epoch 154:\n",
      "Loss: 9.61742\n",
      "Error: 15.625\n",
      "Epoch 155:\n",
      "Loss: 9.69401\n",
      "Error: 23.4375\n",
      "Epoch 156:\n",
      "Loss: 9.76273\n",
      "Error: 25.0\n",
      "Epoch 157:\n",
      "Loss: 9.6323\n",
      "Error: 17.1875\n",
      "Epoch 158:\n",
      "Loss: 9.7561\n",
      "Error: 29.6875\n",
      "Epoch 159:\n",
      "Loss: 9.80004\n",
      "Error: 31.25\n",
      "Epoch 160:\n",
      "Loss: 9.67698\n",
      "Error: 21.875\n",
      "Epoch 161:\n",
      "Loss: 9.76353\n",
      "Error: 28.125\n",
      "Epoch 162:\n",
      "Loss: 9.93146\n",
      "Error: 42.1875\n",
      "Epoch 163:\n",
      "Loss: 9.81339\n",
      "Error: 32.8125\n",
      "Epoch 164:\n",
      "Loss: 9.68905\n",
      "Error: 23.4375\n",
      "Epoch 165:\n",
      "Loss: 9.73839\n",
      "Error: 26.5625\n",
      "Epoch 166:\n",
      "Loss: 9.77263\n",
      "Error: 31.25\n",
      "Epoch 167:\n",
      "Loss: 9.8322\n",
      "Error: 34.375\n",
      "Epoch 168:\n",
      "Loss: 9.88647\n",
      "Error: 37.5\n",
      "Epoch 169:\n",
      "Loss: 9.69995\n",
      "Error: 25.0\n",
      "Epoch 170:\n",
      "Loss: 9.72047\n",
      "Error: 26.5625\n",
      "Epoch 171:\n",
      "Loss: 9.72613\n",
      "Error: 26.5625\n",
      "Epoch 172:\n",
      "Loss: 9.74234\n",
      "Error: 28.125\n",
      "Epoch 173:\n",
      "Loss: 9.62381\n",
      "Error: 17.1875\n",
      "Epoch 174:\n",
      "Loss: 9.7558\n",
      "Error: 26.5625\n",
      "Epoch 175:\n",
      "Loss: 9.69415\n",
      "Error: 23.4375\n",
      "Epoch 176:\n",
      "Loss: 9.65125\n",
      "Error: 20.3125\n",
      "Epoch 177:\n",
      "Loss: 9.73857\n",
      "Error: 26.5625\n",
      "Epoch 178:\n",
      "Loss: 9.62126\n",
      "Error: 17.1875\n",
      "Epoch 179:\n",
      "Loss: 9.81721\n",
      "Error: 34.375\n",
      "Epoch 180:\n",
      "Loss: 9.65305\n",
      "Error: 20.3125\n",
      "Epoch 181:\n",
      "Loss: 9.67163\n",
      "Error: 21.875\n",
      "Epoch 182:\n",
      "Loss: 9.69787\n",
      "Error: 20.3125\n",
      "Epoch 183:\n",
      "Loss: 9.75102\n",
      "Error: 28.125\n",
      "Epoch 184:\n",
      "Loss: 9.70436\n",
      "Error: 25.0\n",
      "Epoch 185:\n",
      "Loss: 9.77039\n",
      "Error: 28.125\n",
      "Epoch 186:\n",
      "Loss: 9.78869\n",
      "Error: 31.25\n",
      "Epoch 187:\n",
      "Loss: 9.6389\n",
      "Error: 20.3125\n",
      "Epoch 188:\n",
      "Loss: 9.79868\n",
      "Error: 34.375\n",
      "Epoch 189:\n",
      "Loss: 9.76283\n",
      "Error: 31.25\n",
      "Epoch 190:\n",
      "Loss: 9.75853\n",
      "Error: 29.6875\n",
      "Epoch 191:\n",
      "Loss: 9.76671\n",
      "Error: 28.125\n",
      "Epoch 192:\n",
      "Loss: 9.69516\n",
      "Error: 26.5625\n",
      "Epoch 193:\n",
      "Loss: 9.70506\n",
      "Error: 21.875\n",
      "Epoch 194:\n",
      "Loss: 9.69432\n",
      "Error: 20.3125\n",
      "Epoch 195:\n",
      "Loss: 9.71391\n",
      "Error: 23.4375\n",
      "Epoch 196:\n",
      "Loss: 9.58704\n",
      "Error: 15.625\n",
      "Epoch 197:\n",
      "Loss: 9.79283\n",
      "Error: 35.9375\n",
      "Epoch 198:\n",
      "Loss: 9.69996\n",
      "Error: 26.5625\n",
      "Epoch 199:\n",
      "Loss: 9.61372\n",
      "Error: 18.75\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i in range(RECORDING_STEP):\n",
    "        batch, labels = get_training_samples(BATCH_SIZE)\n",
    "\n",
    "        feed_dict = { train_data_node: batch\n",
    "                    , train_labels_node: labels}\n",
    "        \n",
    "        s.run(optimizer, feed_dict=feed_dict)\n",
    "\n",
    "    batch, labels = get_training_samples(BATCH_SIZE)\n",
    "    feed_dict = { train_data_node: batch\n",
    "                , train_labels_node: labels}\n",
    "    l, pred = s.run([loss, output], feed_dict=feed_dict)\n",
    "    error_rate = cil.error_rate(pred,labels)\n",
    "    print('Epoch ' + str(epoch) + ':')\n",
    "    print('Loss: ' + str(l))\n",
    "    print('Error: ' + str(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16011849,  0.83988148],\n",
       "       [ 0.25092113,  0.74907893],\n",
       "       [ 0.31138092,  0.68861914],\n",
       "       [ 0.18172739,  0.81827259],\n",
       "       [ 0.17090526,  0.82909471],\n",
       "       [ 0.18933599,  0.81066394],\n",
       "       [ 0.12496678,  0.87503326],\n",
       "       [ 0.28543702,  0.71456301],\n",
       "       [ 0.30342916,  0.69657087],\n",
       "       [ 0.21597187,  0.78402805],\n",
       "       [ 0.2062474 ,  0.79375261],\n",
       "       [ 0.16312912,  0.83687091],\n",
       "       [ 0.26063368,  0.73936635],\n",
       "       [ 0.28800842,  0.71199161],\n",
       "       [ 0.17584823,  0.82415175],\n",
       "       [ 0.25218162,  0.74781841],\n",
       "       [ 0.19763324,  0.80236673],\n",
       "       [ 0.32440129,  0.67559874],\n",
       "       [ 0.19218387,  0.80781609],\n",
       "       [ 0.20473263,  0.79526734],\n",
       "       [ 0.28218603,  0.71781397],\n",
       "       [ 0.28250721,  0.71749276],\n",
       "       [ 0.21245016,  0.78754985],\n",
       "       [ 0.23202856,  0.76797146],\n",
       "       [ 0.31768733,  0.68231267],\n",
       "       [ 0.30259198,  0.69740802],\n",
       "       [ 0.22021489,  0.77978516],\n",
       "       [ 0.18295817,  0.81704187],\n",
       "       [ 0.28748864,  0.71251136],\n",
       "       [ 0.31122437,  0.6887756 ],\n",
       "       [ 0.27371553,  0.7262845 ],\n",
       "       [ 0.25813788,  0.74186212],\n",
       "       [ 0.31568295,  0.68431705],\n",
       "       [ 0.19207264,  0.80792737],\n",
       "       [ 0.38530156,  0.61469847],\n",
       "       [ 0.15217206,  0.84782791],\n",
       "       [ 0.22573657,  0.77426338],\n",
       "       [ 0.31930289,  0.68069708],\n",
       "       [ 0.18727475,  0.81272525],\n",
       "       [ 0.26756307,  0.7324369 ],\n",
       "       [ 0.24225025,  0.75774974],\n",
       "       [ 0.21127982,  0.78872013],\n",
       "       [ 0.2994414 ,  0.7005586 ],\n",
       "       [ 0.22342049,  0.7765795 ],\n",
       "       [ 0.19199516,  0.80800486],\n",
       "       [ 0.21560335,  0.78439659],\n",
       "       [ 0.15660456,  0.84339547],\n",
       "       [ 0.20370121,  0.7962988 ],\n",
       "       [ 0.34743813,  0.65256184],\n",
       "       [ 0.26810598,  0.73189396],\n",
       "       [ 0.24590959,  0.75409043],\n",
       "       [ 0.21550037,  0.7844997 ],\n",
       "       [ 0.23070222,  0.76929784],\n",
       "       [ 0.34905335,  0.65094668],\n",
       "       [ 0.22730932,  0.77269065],\n",
       "       [ 0.16806829,  0.83193171],\n",
       "       [ 0.25212523,  0.7478748 ],\n",
       "       [ 0.27951804,  0.72048199],\n",
       "       [ 0.3111904 ,  0.68880963],\n",
       "       [ 0.31259707,  0.68740296],\n",
       "       [ 0.40300778,  0.59699225],\n",
       "       [ 0.22189189,  0.77810812],\n",
       "       [ 0.19824415,  0.80175585],\n",
       "       [ 0.33787939,  0.66212064]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 48, 48, 6)\n",
      "(1, 24, 24, 32)\n",
      "(1, 12, 12, 64)\n",
      "(1, 12, 12, 64)\n",
      "(1, 9216)\n",
      "(1, 512)\n",
      "(1, 128)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "pred_input = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(1, CONTEXT_SIZE, CONTEXT_SIZE, NUM_CHANNELS),\n",
    "    name='pred_input')\n",
    "pred_output = tf.nn.softmax(model(pred_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with image 0\n",
      "19.52\n"
     ]
    }
   ],
   "source": [
    "imgs = train_data[:1]\n",
    "# assuming images are quadratic\n",
    "num_patches = imgs[0].shape[0] // PATCH_SIZE\n",
    "truth = np.empty((len(imgs), num_patches, num_patches, cil.NUM_LABELS))\n",
    "preds = np.empty((len(imgs), num_patches, num_patches, cil.NUM_LABELS))\n",
    "for img_idx in range(len(imgs)):\n",
    "    img = imgs[img_idx]\n",
    "    for patch_y in range(num_patches):\n",
    "        y = patch_y * PATCH_SIZE - EXTRA_CONTEXT\n",
    "        for patch_x in range(num_patches):\n",
    "            x = patch_x * PATCH_SIZE - EXTRA_CONTEXT\n",
    "            window, window_truth = get_window(img, y, x, train_labels[img_idx])\n",
    "            pred = s.run(pred_output, {pred_input: [window]})\n",
    "            preds[img_idx, patch_y, patch_x] = pred\n",
    "            label = window_truth.mean()\n",
    "            truth[img_idx, patch_y, patch_x, 0] = label\n",
    "            truth[img_idx, patch_y, patch_x, 1] = 1 - label\n",
    "    print('Done with image ' + str(img_idx))\n",
    "truth = truth.reshape((len(imgs) * num_patches * num_patches, cil.NUM_LABELS))\n",
    "preds = preds.reshape((len(imgs) * num_patches * num_patches, cil.NUM_LABELS))\n",
    "error_rate = cil.error_rate(preds, truth)\n",
    "print(error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22945783,  0.77054214],\n",
       "       [ 0.23343855,  0.76656151],\n",
       "       [ 0.23387842,  0.76612163],\n",
       "       ..., \n",
       "       [ 0.30433786,  0.69566208],\n",
       "       [ 0.26381034,  0.73618966],\n",
       "       [ 0.23759136,  0.76240861]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_m = np.empty(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.599999999999994"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_m[:,0] = preds[:,0]+.3\n",
    "preds_m[:,1] = 1 - preds_m[:,0]\n",
    "cil.error_rate(preds_m, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAAAAACl1GkQAAAEAklEQVR4nO3dMW6DQBBA0WzE/a/s\n1JlmZTmBZ/xfhxuwvqYaLazHVyTfVz9AfisIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCY\nY/6wxnX7knM1IZiCYAqCKQimIJiCYAqCKQimIJiCYAqCKQimIJiCYAqCWe07LE0IpiCYgmAKgikI\npiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCY\ngmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAK\ngikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikI\npiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCY\ngmAKgikIpiCYgmCOs2+4xvXjw+6/04RgCoIpCKYgmIJgCoIpCKYgmIJgCoIpCKYgmIJgCoIpCGZp\n+4C/pu8/piYEUxBMQTAFwRQEUxBMQTAFwRQEUxBMQTAFwRQEUxBMQTC334e86ux9ShOCKQimIJiC\nYAqCKQimIJiCYAqCKQimIJiCYAqCKQimIJj2IZgmBFMQTEEwBcEUBFMQTEEwBcEUBFMQTEEwBcEU\nBFMQTEEwBcEUBFMQTEEwBcEUBFMQTEEwBcEUBFMQTEEwBcEUBFMQTEEwBcEUBFMQTEEwBcEUBFMQ\nTEEwBcEUBFMQTEEwBcEUBFMQTEEwx9UP8N/6nnpeUhBMQTAFwRQEUxBMQTAFwRQEUxBMQTAFwRQE\nUxBMQTB9PwTThGAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAK\ngikIpiCYgmAKgikIpiCYgmAKgikIhntf1ny/1XT38yxNCKYgmIJgCoIpCKYgmIJgCoIpCKYgmIJg\nCoIpCKYgmIJg+Pdlfdp+pAnBFARTEExBMAXBFARTEExBMAXBFARTEExBMAXBFARTEAx3PmTnbvuP\nqQnBFARTEExBMAXBFARTEExBMAXBFARTEExBMAXBFARTEAx/PmRnd35k0v9vE4IpCKYgmIJgCoIp\nCKYgmIJgCoIpCKYgmIJgCoIpCKYgmNPPh1z9/qt5f20/0oRgCoIpCKYgmIJgCoIpCKYgmIJgCoIp\nCKYgmIJgCoIpCObtz4fsXL1/eVYTgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCY2+9D\n3k0TgikIpiCYgmAKgikIpiCYgmAKgikIpiCYgmAKgikIpiCYHzRiGhi5oEjGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=400x400 at 0x7FF2BF8E6BE0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_img = cil.label_to_img(imgs[0].shape[0],imgs[0].shape[1],\n",
    "                             PATCH_SIZE,PATCH_SIZE,preds_m)\n",
    "show(label_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction on training set\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_preproc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-ba87233440c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAINING_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mpimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_prediction_with_groundtruth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_preproc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_training_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"prediction_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0moimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_prediction_with_overlay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_preproc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_preproc' is not defined"
     ]
    }
   ],
   "source": [
    "print (\"Running prediction on training set\")\n",
    "prediction_training_dir = \"predictions_training/\"\n",
    "if not os.path.isdir(prediction_training_dir):\n",
    "    os.mkdir(prediction_training_dir)\n",
    "    \n",
    "for i in range(TRAINING_SIZE):\n",
    "    pimg = get_prediction_with_groundtruth(train_preproc[i])\n",
    "    Image.fromarray(pimg).save(prediction_training_dir + \"prediction_\" + str(i+1) + \".png\")\n",
    "    oimg = get_prediction_with_overlay(train_preproc[i])\n",
    "    oimg.save(prediction_training_dir + \"overlay-8x8_\" + str(i+1) + \".png\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Running prediction on test set\")\n",
    "prediction_test_dir = \"predictions_test/\"\n",
    "if not os.path.isdir(prediction_test_dir):\n",
    "    os.mkdir(prediction_test_dir)\n",
    "\n",
    "test_preproc = load_preproc(test_dir)\n",
    "for i in range(TEST_SIZE):\n",
    "    pred = get_prediction(test_preproc[i])\n",
    "    pimg = cil.img_float_to_uint8(pred)\n",
    "    Image.fromarray(pimg).save(prediction_test_dir + \"prediction_\" + str(i+1) + \".png\")\n",
    "    oimg = cil.make_img_overlay(test_preproc[i], pred)\n",
    "    oimg.save(prediction_test_dir + \"overlay-8x8_\" + str(i+1) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "th = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assign a label to a patch\n",
    "def patch_to_label(patch):\n",
    "    df = np.mean(patch)\n",
    "    if df > th:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def mask_to_submission_strings(image_filename):\n",
    "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "    img_number = int(re.search(r\"\\d+\", image_filename).group(0))\n",
    "    im = mpimg.imread(image_filename)\n",
    "    patch_size = 16\n",
    "    for j in range(0, im.shape[1], patch_size):\n",
    "        for i in range(0, im.shape[0], patch_size):\n",
    "            patch = im[i:i + patch_size, j:j + patch_size]\n",
    "            label = patch_to_label(patch)\n",
    "            yield(\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "\n",
    "\n",
    "def masks_to_submission(submission_filename, *image_filenames):\n",
    "    \"\"\"Converts images into a submission file\"\"\"\n",
    "    with open(submission_filename, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        for fn in image_filenames[0:]:\n",
    "            f.writelines('{}\\n'.format(s) for s in mask_to_submission_strings(fn))\n",
    "\n",
    "\n",
    "def save(submission_filename):\n",
    "    #subm#ission_filename = 'submission_07.csv'\n",
    "    image_filenames = []\n",
    "    for i in range(1, 51):\n",
    "        imagename = 'prediction_' + str(i)\n",
    "        image_filename = 'predictions_test/' + imagename + '.png'\n",
    "        print(image_filename)\n",
    "        image_filenames.append(image_filename)\n",
    "    masks_to_submission(submission_filename, *image_filenames)\n",
    "    \n",
    "save('submission_lukas_11_2.5.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvlis",
   "language": "python",
   "name": "venvlis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
