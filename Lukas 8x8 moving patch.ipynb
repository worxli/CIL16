{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Baseline for CIL project on road segmentation.\n",
    "This simple baseline consits of a CNN with two convolutional+pooling layers with a soft-max loss\n",
    "\"\"\"\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from random import shuffle\n",
    "import time\n",
    "import math\n",
    "import code\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import cil_helper as cil\n",
    "\n",
    "from skimage.feature import greycomatrix, greycoprops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tensorflow_lukas_balz\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.app.flags.DEFINE_string('log_dir', '/tmp/tensorflow_lukas_balz',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "except: \n",
    "    print(tf.app.flags.FLAGS.log_dir)\n",
    "    \n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 20 # 100\n",
    "TEST_SIZE = 50 # 50\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 32 # 64\n",
    "NUM_EPOCHS = 20\n",
    "RESTORE_MODEL = False # If True, restore existing model instead of training a new one\n",
    "RECORDING_STEP = 200\n",
    "ERROR_STEP = 500\n",
    "\n",
    "# Set image patch size in pixels\n",
    "# IMG_PATCH_SIZE should be a multiple of 4\n",
    "# image size should be an integer multiple of this number!\n",
    "IMG_PATCH_SIZE = 8\n",
    "NUM_CHANNELS = 6\n",
    "SAMPLING_STEP = 8 #math.ceil(math.sqrt(IMG_PATCH_SIZE))\n",
    "NUM_LABELS = 2\n",
    "\n",
    "# all prediction size\n",
    "ALL_PER = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'training/'\n",
    "train_data_filename = data_dir + 'images/'\n",
    "train_labels_filename = data_dir + 'groundtruth/' \n",
    "test_dir = \"test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_crop(im, size):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    step = SAMPLING_STEP\n",
    "    for i in range(0,imgheight,step):\n",
    "        if i+size <=imgheight:\n",
    "            for j in range(0,imgwidth,step):\n",
    "                if j+size <=imgwidth:\n",
    "                    if is_2d:\n",
    "                        im_patch = im[j:j+size, i:i+size]\n",
    "                    else:\n",
    "                        im_patch = im[j:j+size, i:i+size, :]\n",
    "                    list_patches.append(im_patch)\n",
    "                    list_patches.append(np.rot90(im_patch))\n",
    "                    list_patches.append(np.fliplr(im_patch))\n",
    "    return list_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract label images\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            #print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            gt_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(gt_imgs)\n",
    "    gt_patches = [img_crop(gt_imgs[i], IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "    labels = np.asarray([value_to_class(data[i]) for i in range(len(data))])\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_patches(imgs, TRAINING_SIZE):\n",
    "    num_images = TRAINING_SIZE\n",
    "\n",
    "    img_patches = [img_crop(imgs[i], IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = [img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))]\n",
    "\n",
    "    return np.asarray(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assign a label to a patch v\n",
    "def value_to_class(v):\n",
    "    df = np.sum(np.mean(v))\n",
    "    if df < foreground_threshold:\n",
    "       return [0,1]\n",
    "    else:\n",
    "       return [1,0]\n",
    "    \n",
    "    \n",
    "    size = v.shape[0]\n",
    "    step = int(size/2)\n",
    "    labels = []\n",
    "    for i in range(0,size,step):\n",
    "            for j in range(0,size,step):\n",
    "                labels.append(get_label(np.mean(v[j:j+step, i:i+step])))\n",
    "    return labels\n",
    "\n",
    "def get_label(v):\n",
    "    foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "    df = np.sum(v)\n",
    "    return df\n",
    "    if df < foreground_threshold:\n",
    "       return 0\n",
    "    else:\n",
    "       return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_preproc(save_dir):\n",
    "    assert save_dir.endswith('/')\n",
    "    return np.load(save_dir + 'preprocessed2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_preproc = load_preproc(train_data_filename)\n",
    "train_data = make_patches(train_preproc, TRAINING_SIZE)\n",
    "\n",
    "train_labels = extract_labels(train_labels_filename, TRAINING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144060, 16, 16, 6)\n",
      "(144060, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "#NUM_CHANNELS = 3\n",
    "#train_data = train_data[:,:,:,0:NUM_CHANNELS]\n",
    "#print(train_data.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points per class: c0 = 24834 c1 = 119226\n"
     ]
    }
   ],
   "source": [
    "c0 = 0\n",
    "c1 = 0\n",
    "for i in range(len(train_labels)):\n",
    "    if np.mean(train_labels[i]) > 0.5:\n",
    "        c0 = c0 + 1 # foreground -> road\n",
    "    else:\n",
    "        c1 = c1 + 1 # background\n",
    "print ('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampling = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing training data...\n",
      "49668\n",
      "(49668, 16, 16, 6)\n"
     ]
    }
   ],
   "source": [
    "print ('Balancing training data...')\n",
    "min_c = min(c0, c1)\n",
    "max_c = max(c0, c1)\n",
    "idx0 = [i for i, j in enumerate(train_labels) if np.mean(j) > 0.5]\n",
    "idx1 = [i for i, j in enumerate(train_labels) if np.mean(j) <= 0]\n",
    "\n",
    "if sampling: #sample smaller class\n",
    "    if c0 < c1:\n",
    "        while len(idx0) < c1:\n",
    "            print(len(idx0))\n",
    "            idx0.extend(idx0)\n",
    "            print(len(idx0))\n",
    "    else:\n",
    "        while len(idx1)<c0:\n",
    "            idx1.extend(idx1)\n",
    "    lim = max_c\n",
    "else: \n",
    "    lim = min_c\n",
    "    \n",
    "new_indices = idx0[0:lim] + idx1[0:lim]\n",
    "train_data = train_data[new_indices,:,:,:]\n",
    "train_labels = train_labels[new_indices]\n",
    "    \n",
    "print(len(new_indices))\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = train_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is where training samples and labels are fed to the graph.\n",
    "# These placeholder nodes will be fed a batch of training data at each\n",
    "# training step using the {feed_dict} argument to the Run() call below.\n",
    "train_data_node = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(BATCH_SIZE, IMG_PATCH_SIZE, IMG_PATCH_SIZE, NUM_CHANNELS),\n",
    "    name='train_data')\n",
    "train_labels_node = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(BATCH_SIZE, NUM_LABELS),\n",
    "    name='train_labels')\n",
    "train_all_data_node = tf.constant(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The variables below hold all the trainable weights. They are passed an\n",
    "# initial value which will be assigned when when we call:\n",
    "# {tf.initialize_all_variables().run()}\n",
    "conv1_1_weights = cil.weight_variable([4, 4, NUM_CHANNELS, 20])\n",
    "conv1_1_biases = cil.bias_variable([20])\n",
    "conv1_2_weights = cil.weight_variable([4, 4, 20, 20])\n",
    "conv1_2_biases = cil.bias_variable([20])\n",
    "\n",
    "conv2_weights = cil.weight_variable([4, 4, 20, 40])\n",
    "conv2_biases = cil.bias_variable([40])\n",
    "\n",
    "conv3_weights = cil.weight_variable([4, 4, 40, 80])\n",
    "conv3_biases = cil.bias_variable([80])\n",
    "\n",
    "conv4_size = 256\n",
    "conv4_weights = cil.weight_variable([2, 2, 160, conv4_size])\n",
    "conv4_biases = cil.bias_variable([conv4_size])\n",
    "\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "    tf.truncated_normal([640 , 512],\n",
    "                        stddev=0.1,\n",
    "                        seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "fc2_weights = tf.Variable(\n",
    "    tf.truncated_normal([512, NUM_LABELS],\n",
    "                        stddev=0.1,\n",
    "                        seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will replicate the model structure for the training subgraph, as well\n",
    "# as the evaluation subgraphs, while sharing the trainable parameters.\n",
    "def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    conv1_1 = cil.conv2d(data, conv1_1_weights)\n",
    "    relu1_1 = tf.nn.relu(tf.nn.bias_add(conv1_1, conv1_1_biases))\n",
    "    #conv1_2 = cil.conv2d(relu1_1, conv1_2_weights)\n",
    "    #relu1_2 = tf.nn.relu(tf.nn.bias_add(conv1_2, conv1_2_biases))\n",
    "    \n",
    "    pool1 = cil.max_pool_2x2(relu1_1)\n",
    "\n",
    "    conv2 = cil.conv2d(pool1, conv2_weights)\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "    pool2 = cil.max_pool_2x2(relu2)\n",
    "    \n",
    "    conv3 = cil.conv2d(pool2, conv3_weights)\n",
    "    relu3 = tf.nn.relu(tf.nn.bias_add(conv3, conv3_biases))\n",
    "    maxpool = cil.max_pool_2x2(relu3)\n",
    "    avgpool = cil.avg_pool_2x2(relu3)\n",
    "    pool3 = tf.concat(3, [maxpool, avgpool])\n",
    "    #print(maxpool.get_shape())\n",
    "    #print(avgpool.get_shape())\n",
    "    #print(pool3.get_shape())\n",
    "    \n",
    "    conv4 = cil.conv2d(pool3, conv4_weights)\n",
    "    relu4 = tf.nn.relu(tf.nn.bias_add(conv4, conv4_biases))\n",
    "    pool4 = cil.max_pool_2x2(relu4)\n",
    "\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool2.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool2,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    \n",
    "    #print(pool4.get_shape())\n",
    "    #print(reshape.get_shape())\n",
    "    #print(fc1_weights.get_shape())\n",
    "    \n",
    "    fc1 = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "    if train:\n",
    "        fc1 = tf.nn.dropout(fc1, 0.5, seed=SEED)\n",
    "    fc2 = tf.matmul(fc1, fc2_weights) + fc2_biases\n",
    "\n",
    "    return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True) # BATCH_SIZE*NUM_LABELS\n",
    "# print 'logits = ' + str(logits.get_shape()) + ' train_labels_node = ' + str(train_labels_node.get_shape())\n",
    "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    " #   logits, train_labels_node))\n",
    "loss = tf.reduce_mean(tf.square(logits - train_labels_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    0.01,                # Base learning rate.\n",
    "    batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "    train_size,          # Decay step.\n",
    "    0.95,                # Decay rate.\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch)\n",
    "\n",
    "# try adam\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=batch)\n",
    "\n",
    "#optimizer = tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predictions for the minibatch, validation set and test set.\n",
    "train_prediction = logits #tf.nn.softmax(logits)\n",
    "# We'll compute them only once in a while by calling their {eval()} method.\n",
    "train_all_prediction = model(train_all_data_node[0:ALL_PER,:,:,:]) #tf.nn.softmax(model(train_all_data_node[0:ALL_PER,:,:,:]))\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f2bff43c400>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leo/.venvlis/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 171, in __del__\n",
      "    self.close()\n",
      "  File \"/home/leo/.venvlis/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 976, in close\n",
      "    self._default_session.__exit__(None, None, None)\n",
      "  File \"/usr/lib/python3.4/contextlib.py\", line 66, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/home/leo/.venvlis/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 3378, in get_controller\n",
      "    % type(default))\n",
      "AssertionError: Nesting violated for default stack of <class 'weakref'> objects\n"
     ]
    }
   ],
   "source": [
    "s = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if RESTORE_MODEL:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(s,FLAGS.train_dir + \"/model8.ckpt\")\n",
    "    print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#saver.restore(s,FLAGS.train_dir + \"/model8.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n"
     ]
    }
   ],
   "source": [
    "# Run all the initializers to prepare the trainable parameters.\n",
    "tf.initialize_all_variables().run()\n",
    "print ('Initialized!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of iterations = 31042\n",
      "step 0, error: 79.0% (0.122 sec/batch)\n",
      "step 200, batch error: 81.2% (0.058 sec/batch)\n",
      "step 400, batch error: 78.1% (0.010 sec/batch)\n",
      "step 500, error: 42.0% (0.022 sec/batch)\n",
      "step 600, batch error: 65.6% (0.010 sec/batch)\n",
      "step 800, batch error: 81.2% (0.013 sec/batch)\n",
      "step 1000, error: 62.0% (0.025 sec/batch)\n",
      "step 1200, batch error: 84.4% (0.010 sec/batch)\n",
      "step 1400, batch error: 81.2% (0.010 sec/batch)\n",
      "step 1500, error: 63.0% (0.025 sec/batch)\n",
      "step 1600, batch error: 81.2% (0.011 sec/batch)\n",
      "step 1800, batch error: 68.8% (0.012 sec/batch)\n",
      "step 2000, error: 66.0% (0.032 sec/batch)\n",
      "step 2200, batch error: 81.2% (0.012 sec/batch)\n",
      "step 2400, batch error: 81.2% (0.009 sec/batch)\n",
      "step 2500, error: 79.0% (0.055 sec/batch)\n",
      "step 2600, batch error: 87.5% (0.011 sec/batch)\n",
      "step 2800, batch error: 75.0% (0.010 sec/batch)\n",
      "step 3000, error: 70.0% (0.024 sec/batch)\n",
      "step 3200, batch error: 65.6% (0.010 sec/batch)\n",
      "step 3400, batch error: 75.0% (0.009 sec/batch)\n",
      "step 3500, error: 59.0% (0.247 sec/batch)\n",
      "step 3600, batch error: 93.8% (0.012 sec/batch)\n",
      "step 3800, batch error: 71.9% (0.012 sec/batch)\n",
      "step 4000, error: 70.0% (0.030 sec/batch)\n",
      "step 4200, batch error: 59.4% (0.012 sec/batch)\n",
      "step 4400, batch error: 68.8% (0.012 sec/batch)\n",
      "step 4500, error: 74.0% (0.024 sec/batch)\n",
      "step 4600, batch error: 84.4% (0.011 sec/batch)\n",
      "step 4800, batch error: 68.8% (0.012 sec/batch)\n",
      "step 5000, error: 42.0% (0.026 sec/batch)\n",
      "step 5200, batch error: 75.0% (0.011 sec/batch)\n",
      "step 5400, batch error: 81.2% (0.010 sec/batch)\n",
      "step 5500, error: 70.0% (0.024 sec/batch)\n",
      "step 5600, batch error: 59.4% (0.011 sec/batch)\n",
      "step 5800, batch error: 84.4% (0.012 sec/batch)\n",
      "step 6000, error: 73.0% (0.028 sec/batch)\n",
      "step 6200, batch error: 78.1% (0.007 sec/batch)\n",
      "step 6400, batch error: 75.0% (0.009 sec/batch)\n",
      "step 6500, error: 64.0% (0.027 sec/batch)\n",
      "step 6600, batch error: 71.9% (0.009 sec/batch)\n",
      "step 6800, batch error: 68.8% (0.009 sec/batch)\n",
      "step 7000, error: 71.0% (0.024 sec/batch)\n",
      "step 7200, batch error: 71.9% (0.013 sec/batch)\n",
      "step 7400, batch error: 71.9% (0.011 sec/batch)\n",
      "step 7500, error: 55.0% (0.023 sec/batch)\n",
      "step 7600, batch error: 68.8% (0.013 sec/batch)\n",
      "step 7800, batch error: 81.2% (0.010 sec/batch)\n",
      "step 8000, error: 65.0% (0.022 sec/batch)\n",
      "step 8200, batch error: 71.9% (0.010 sec/batch)\n",
      "step 8400, batch error: 59.4% (0.010 sec/batch)\n",
      "step 8500, error: 65.0% (0.023 sec/batch)\n",
      "step 8600, batch error: 56.2% (0.010 sec/batch)\n",
      "step 8800, batch error: 65.6% (0.011 sec/batch)\n",
      "step 9000, error: 63.0% (0.024 sec/batch)\n",
      "step 9200, batch error: 59.4% (0.008 sec/batch)\n",
      "step 9400, batch error: 71.9% (0.011 sec/batch)\n",
      "step 9500, error: 54.0% (0.024 sec/batch)\n",
      "step 9600, batch error: 65.6% (0.016 sec/batch)\n",
      "step 9800, batch error: 71.9% (0.010 sec/batch)\n",
      "step 10000, error: 66.0% (0.029 sec/batch)\n",
      "step 10200, batch error: 81.2% (0.011 sec/batch)\n",
      "step 10400, batch error: 84.4% (0.009 sec/batch)\n",
      "step 10500, error: 56.0% (0.022 sec/batch)\n",
      "step 10600, batch error: 71.9% (0.010 sec/batch)\n",
      "step 10800, batch error: 56.2% (0.011 sec/batch)\n",
      "step 11000, error: 64.0% (0.025 sec/batch)\n",
      "step 11200, batch error: 75.0% (0.010 sec/batch)\n",
      "step 11400, batch error: 81.2% (0.009 sec/batch)\n",
      "step 11500, error: 57.0% (0.030 sec/batch)\n",
      "step 11600, batch error: 93.8% (0.012 sec/batch)\n",
      "step 11800, batch error: 71.9% (0.012 sec/batch)\n",
      "step 12000, error: 36.0% (0.027 sec/batch)\n",
      "step 12200, batch error: 65.6% (0.010 sec/batch)\n",
      "step 12400, batch error: 75.0% (0.010 sec/batch)\n",
      "step 12500, error: 74.0% (0.025 sec/batch)\n",
      "step 12600, batch error: 84.4% (0.009 sec/batch)\n",
      "step 12800, batch error: 75.0% (0.010 sec/batch)\n",
      "step 13000, error: 57.0% (0.024 sec/batch)\n",
      "step 13200, batch error: 75.0% (0.017 sec/batch)\n",
      "step 13400, batch error: 71.9% (0.011 sec/batch)\n",
      "step 13500, error: 50.0% (0.025 sec/batch)\n",
      "step 13600, batch error: 65.6% (0.013 sec/batch)\n",
      "step 13800, batch error: 71.9% (0.012 sec/batch)\n",
      "step 14000, error: 63.0% (0.030 sec/batch)\n",
      "step 14200, batch error: 81.2% (0.014 sec/batch)\n",
      "step 14400, batch error: 75.0% (0.012 sec/batch)\n",
      "step 14500, error: 52.0% (0.025 sec/batch)\n",
      "step 14600, batch error: 62.5% (0.010 sec/batch)\n",
      "step 14800, batch error: 84.4% (0.011 sec/batch)\n",
      "step 15000, error: 58.0% (0.025 sec/batch)\n",
      "step 15200, batch error: 81.2% (0.011 sec/batch)\n",
      "step 15400, batch error: 71.9% (0.011 sec/batch)\n",
      "step 15500, error: 48.0% (0.025 sec/batch)\n",
      "step 15600, batch error: 78.1% (0.029 sec/batch)\n",
      "step 15800, batch error: 68.8% (0.014 sec/batch)\n",
      "step 16000, error: 59.0% (0.020 sec/batch)\n",
      "step 16200, batch error: 68.8% (0.011 sec/batch)\n",
      "step 16400, batch error: 71.9% (0.011 sec/batch)\n",
      "step 16500, error: 57.0% (0.025 sec/batch)\n",
      "step 16600, batch error: 84.4% (0.012 sec/batch)\n",
      "step 16800, batch error: 68.8% (0.012 sec/batch)\n",
      "step 17000, error: 60.0% (0.025 sec/batch)\n",
      "step 17200, batch error: 84.4% (0.009 sec/batch)\n",
      "step 17400, batch error: 75.0% (0.012 sec/batch)\n",
      "step 17500, error: 45.0% (0.027 sec/batch)\n",
      "step 17600, batch error: 71.9% (0.011 sec/batch)\n",
      "step 17800, batch error: 62.5% (0.013 sec/batch)\n",
      "step 18000, error: 60.0% (0.034 sec/batch)\n",
      "step 18200, batch error: 65.6% (0.013 sec/batch)\n",
      "step 18400, batch error: 75.0% (0.014 sec/batch)\n",
      "step 18500, error: 52.0% (0.086 sec/batch)\n",
      "step 18600, batch error: 75.0% (0.016 sec/batch)\n",
      "step 18800, batch error: 68.8% (0.036 sec/batch)\n",
      "step 19000, error: 57.0% (0.025 sec/batch)\n",
      "step 19200, batch error: 65.6% (0.011 sec/batch)\n",
      "step 19400, batch error: 71.9% (0.009 sec/batch)\n",
      "step 19500, error: 43.0% (0.025 sec/batch)\n",
      "step 19600, batch error: 59.4% (0.010 sec/batch)\n",
      "step 19800, batch error: 68.8% (0.010 sec/batch)\n",
      "step 20000, error: 58.0% (0.029 sec/batch)\n",
      "step 20200, batch error: 68.8% (0.011 sec/batch)\n",
      "step 20400, batch error: 78.1% (0.009 sec/batch)\n",
      "step 20500, error: 46.0% (0.037 sec/batch)\n",
      "step 20600, batch error: 81.2% (0.011 sec/batch)\n",
      "step 20800, batch error: 68.8% (0.017 sec/batch)\n",
      "step 21000, error: 58.0% (0.031 sec/batch)\n",
      "step 21200, batch error: 59.4% (0.012 sec/batch)\n",
      "step 21400, batch error: 65.6% (0.011 sec/batch)\n",
      "step 21500, error: 58.0% (0.025 sec/batch)\n",
      "step 21600, batch error: 62.5% (0.011 sec/batch)\n",
      "step 21800, batch error: 78.1% (0.011 sec/batch)\n",
      "step 22000, error: 62.0% (0.031 sec/batch)\n",
      "step 22200, batch error: 78.1% (0.009 sec/batch)\n",
      "step 22400, batch error: 71.9% (0.015 sec/batch)\n",
      "step 22500, error: 66.0% (0.027 sec/batch)\n",
      "step 22600, batch error: 50.0% (0.011 sec/batch)\n",
      "step 22800, batch error: 71.9% (0.012 sec/batch)\n",
      "step 23000, error: 60.0% (0.027 sec/batch)\n",
      "step 23200, batch error: 71.9% (0.010 sec/batch)\n",
      "step 23400, batch error: 71.9% (0.011 sec/batch)\n",
      "step 23500, error: 61.0% (0.024 sec/batch)\n",
      "step 23600, batch error: 75.0% (0.011 sec/batch)\n",
      "step 23800, batch error: 68.8% (0.007 sec/batch)\n",
      "step 24000, error: 49.0% (0.024 sec/batch)\n",
      "step 24200, batch error: 81.2% (0.019 sec/batch)\n",
      "step 24400, batch error: 81.2% (0.007 sec/batch)\n",
      "step 24500, error: 54.0% (0.024 sec/batch)\n",
      "step 24600, batch error: 78.1% (0.014 sec/batch)\n",
      "step 24800, batch error: 68.8% (0.010 sec/batch)\n",
      "step 25000, error: 36.0% (0.023 sec/batch)\n",
      "step 25200, batch error: 65.6% (0.010 sec/batch)\n",
      "step 25400, batch error: 78.1% (0.007 sec/batch)\n",
      "step 25500, error: 39.0% (0.027 sec/batch)\n",
      "step 25600, batch error: 78.1% (0.011 sec/batch)\n",
      "step 25800, batch error: 71.9% (0.013 sec/batch)\n",
      "step 26000, error: 48.0% (0.021 sec/batch)\n",
      "step 26200, batch error: 62.5% (0.010 sec/batch)\n",
      "step 26400, batch error: 68.8% (0.010 sec/batch)\n",
      "step 26500, error: 54.0% (0.025 sec/batch)\n",
      "step 26600, batch error: 78.1% (0.012 sec/batch)\n",
      "step 26800, batch error: 56.2% (0.010 sec/batch)\n",
      "step 27000, error: 50.0% (0.024 sec/batch)\n",
      "step 27200, batch error: 62.5% (0.013 sec/batch)\n",
      "step 27400, batch error: 71.9% (0.031 sec/batch)\n",
      "step 27500, error: 48.0% (0.020 sec/batch)\n",
      "step 27600, batch error: 68.8% (0.010 sec/batch)\n",
      "step 27800, batch error: 84.4% (0.010 sec/batch)\n",
      "step 28000, error: 56.0% (0.025 sec/batch)\n",
      "step 28200, batch error: 59.4% (0.011 sec/batch)\n",
      "step 28400, batch error: 71.9% (0.007 sec/batch)\n",
      "step 28500, error: 64.0% (0.026 sec/batch)\n",
      "step 28600, batch error: 68.8% (0.010 sec/batch)\n",
      "step 28800, batch error: 81.2% (0.011 sec/batch)\n",
      "step 29000, error: 53.0% (0.025 sec/batch)\n",
      "step 29200, batch error: 59.4% (0.010 sec/batch)\n",
      "step 29400, batch error: 81.2% (0.013 sec/batch)\n",
      "step 29500, error: 57.0% (0.026 sec/batch)\n",
      "step 29600, batch error: 75.0% (0.010 sec/batch)\n",
      "step 29800, batch error: 68.8% (0.010 sec/batch)\n",
      "step 30000, error: 57.0% (0.024 sec/batch)\n",
      "step 30200, batch error: 84.4% (0.011 sec/batch)\n",
      "step 30400, batch error: 78.1% (0.015 sec/batch)\n",
      "step 30500, error: 58.0% (0.031 sec/batch)\n",
      "step 30600, batch error: 71.9% (0.012 sec/batch)\n",
      "step 30800, batch error: 84.4% (0.011 sec/batch)\n",
      "step 31000, error: 59.0% (0.028 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "# Loop through training steps.\n",
    "print ('Total number of iterations = ' + str(int(NUM_EPOCHS * train_size / BATCH_SIZE)))\n",
    "\n",
    "training_indices = range(train_size)\n",
    "\n",
    "for iepoch in range(NUM_EPOCHS):\n",
    "\n",
    "    # Permute training indices\n",
    "    perm_indices = np.random.permutation(training_indices)\n",
    "\n",
    "    for step in range (int(train_size / BATCH_SIZE)):\n",
    "\n",
    "        offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "        batch_indices = perm_indices[offset:(offset + BATCH_SIZE)]\n",
    "\n",
    "        # Compute the offset of the current minibatch in the data.\n",
    "        # Note that we could use better randomization across epochs.\n",
    "        batch_data = train_data[batch_indices, :, :, :]\n",
    "        batch_labels = train_labels[batch_indices]\n",
    "        \n",
    "        \n",
    "        #print('batch_data.shape = ' + str(batch_data.shape))\n",
    "        # This dictionary maps the batch data (as a numpy array) to the\n",
    "        # node in the graph is should be fed to.\n",
    "        feed_dict = {train_data_node: batch_data, train_labels_node: batch_labels}\n",
    "        \n",
    "        stepNr = step + iepoch*int(train_size / BATCH_SIZE)\n",
    "\n",
    "        if stepNr % ERROR_STEP == 0:\n",
    "            start_time = time.time()\n",
    "            l, lr, all_predictions = s.run(\n",
    "                        [loss, learning_rate, train_all_prediction],\n",
    "                        feed_dict=feed_dict)\n",
    "            \n",
    "            duration = time.time() - start_time\n",
    "            #print ('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "            #print ('Minibatch error: %.1f%%' % cil.error_rate(predictions,\n",
    "                                                              #   batch_labels))\n",
    "            loss_value = cil.error_rate(all_predictions, train_labels[:ALL_PER,:])\n",
    "            \n",
    "            format_str = ('step %d, error: %.1f%% (%.3f sec/batch)')\n",
    "            print (format_str % (stepNr, loss_value, duration)) \n",
    "        elif stepNr % RECORDING_STEP == 0:\n",
    "            \n",
    "            start_time = time.time()\n",
    "            l, lr, all_predictions = s.run(\n",
    "                        [loss, learning_rate, train_prediction],\n",
    "                        feed_dict=feed_dict)\n",
    "            \n",
    "            duration = time.time() - start_time\n",
    "            #print ('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "            #print ('Minibatch error: %.1f%%' % cil.error_rate(predictions,\n",
    "                                                              #   batch_labels))\n",
    "            loss_value = cil.error_rate(predictions, batch_labels)\n",
    "            \n",
    "            format_str = ('step %d, batch error: %.1f%% (%.3f sec/batch)')\n",
    "            print (format_str % (stepNr, loss_value, duration)) \n",
    "        else:\n",
    "            # Run the graph and fetch some of the nodes.\n",
    "            _, l, lr, predictions = s.run(\n",
    "                        [optimizer, loss, learning_rate, train_prediction],\n",
    "                        feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_path = saver.save(s, FLAGS.train_dir + \"/model8.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert array of labels to an image\n",
    "def label_to_img(imgsize, size, labels):\n",
    "    array_labels = np.zeros([imgsize, imgsize])\n",
    "    idx = 0\n",
    "    step = size #SAMPLING_STEP\n",
    "    for i in range(0,imgsize,step):\n",
    "        if i+size <=imgsize:\n",
    "            for j in range(0,imgsize,step):\n",
    "                if j+size <=imgsize:\n",
    "                    ## patch has 4 labels\n",
    "                    patch = array_labels[j:j+size, i:i+size]\n",
    "                    # get the 4 classes\n",
    "                    array_labels[j:j+size, i:i+size] = (array_labels[j:j+size, i:i+size] + class_to_img(patch, labels[idx]))/2\n",
    "                    idx = idx + 1\n",
    "    return array_labels\n",
    "\n",
    "# Assign a label to a patch v\n",
    "def class_to_img(v, labels):\n",
    "    size = v.shape[0]\n",
    "    step = int(size/2)\n",
    "    idx = 0\n",
    "    for i in range(0,size,step):\n",
    "            for j in range(0,size,step):\n",
    "                if labels[idx] > 0.5:\n",
    "                    l = 1\n",
    "                else:\n",
    "                    l = 0\n",
    "                v[j:j+step, i:i+step] = l\n",
    "                idx = idx + 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get prediction overlaid on the original image for given input image\n",
    "def get_prediction_with_overlay(img):\n",
    "    \n",
    "    img_prediction = get_prediction(img)\n",
    "    oimg = cil.make_img_overlay(img[:,:,:3], img_prediction)\n",
    "\n",
    "    return oimg\n",
    "\n",
    "# Get a concatenation of the prediction and groundtruth for given input file\n",
    "def get_prediction_with_groundtruth(img):\n",
    "    \n",
    "    img_prediction = get_prediction(img)\n",
    "    cimg = cil.concatenate_images(img[:,:,:3], img_prediction)\n",
    "\n",
    "    return cimg\n",
    "\n",
    "# Get prediction for given input image \n",
    "def get_prediction(img):\n",
    "    data = np.asarray(img_crop(img, IMG_PATCH_SIZE))\n",
    "    data = data[:,:,:,0:NUM_CHANNELS]\n",
    "    data_node = tf.constant(data)\n",
    "    output = model(data_node) #tf.nn.softmax(model(data_node))\n",
    "    output_prediction = s.run(output)\n",
    "    img_prediction = label_to_img(img.shape[0], IMG_PATCH_SIZE, output_prediction)\n",
    "    #img_prediction = np.rint(img_prediction/np.max(img_prediction))\n",
    "\n",
    "    return img_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction on training set\n"
     ]
    }
   ],
   "source": [
    "print (\"Running prediction on training set\")\n",
    "prediction_training_dir = \"predictions_training/\"\n",
    "if not os.path.isdir(prediction_training_dir):\n",
    "    os.mkdir(prediction_training_dir)\n",
    "    \n",
    "for i in range(TRAINING_SIZE):\n",
    "    pimg = get_prediction_with_groundtruth(train_preproc[i])\n",
    "    Image.fromarray(pimg).save(prediction_training_dir + \"prediction8_\" + str(i+1) + \".png\")\n",
    "    oimg = get_prediction_with_overlay(train_preproc[i])\n",
    "    oimg.save(prediction_training_dir + \"overlay8_\" + str(i+1) + \".png\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction on test set\n"
     ]
    }
   ],
   "source": [
    "print (\"Running prediction on test set\")\n",
    "prediction_test_dir = \"predictions_test/\"\n",
    "if not os.path.isdir(prediction_test_dir):\n",
    "    os.mkdir(prediction_test_dir)\n",
    "\n",
    "test_preproc = load_preproc(test_dir)\n",
    "for i in range(TEST_SIZE):\n",
    "    pred = get_prediction(test_preproc[i])\n",
    "    pimg = cil.img_float_to_uint8(pred)\n",
    "    Image.fromarray(pimg).save(prediction_test_dir + \"prediction8_\" + str(i+1) + \".png\")\n",
    "    oimg = get_prediction_with_overlay(test_preproc[i])\n",
    "    oimg.save(prediction_test_dir + \"overlay8_\" + str(i+1) + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "th = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assign a label to a patch\n",
    "def patch_to_label(patch):\n",
    "    df = np.mean(patch)\n",
    "    if df > th:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def mask_to_submission_strings(image_filename):\n",
    "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "    reg = re.search(r\"\\_\\d+\", image_filename)\n",
    "    group = reg.group(0)[1:]\n",
    "    img_number = int(group)\n",
    "    im = mpimg.imread(image_filename)\n",
    "    patch_size = 16\n",
    "    for j in range(0, im.shape[1], patch_size):\n",
    "        for i in range(0, im.shape[0], patch_size):\n",
    "            patch = im[i:i + patch_size, j:j + patch_size]\n",
    "            label = patch_to_label(patch)\n",
    "            yield(\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "\n",
    "\n",
    "def masks_to_submission(submission_filename, *image_filenames):\n",
    "    \"\"\"Converts images into a submission file\"\"\"\n",
    "    with open(submission_filename, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        for fn in image_filenames[0:]:\n",
    "            f.writelines('{}\\n'.format(s) for s in mask_to_submission_strings(fn))\n",
    "\n",
    "\n",
    "def save(submission_filename):\n",
    "    #subm#ission_filename = 'submission_07.csv'\n",
    "    image_filenames = []\n",
    "    for i in range(1, TEST_SIZE+1):\n",
    "        imagename = 'prediction8_' + str(i)\n",
    "        image_filename = 'predictions_test/' + imagename + '.png'\n",
    "        print(image_filename)\n",
    "        image_filenames.append(image_filename)\n",
    "    masks_to_submission(submission_filename, *image_filenames)\n",
    "    \n",
    "save('submission_lukas_8x8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvlis",
   "language": "python",
   "name": "venvlis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
